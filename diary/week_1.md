# Feats

- Reading the full paper on the original GAN paper, and understanding the proofs and concepts behind it.
    This principle allowed me to take an indepth approach towards discovering new models.
- The principle behind reading the paper about GANs, lead me to the introduction to different varients, DCGANs-
    CDCGans, WGANs and more, but they all base them self from the original paper.
- Looked into papers regarding active label synthesis and how labeling works with gans, quite facinating, when you link
the maths up.

# Observations

- Maths behind generative networks are very heavy, and there are no papers that can purely verify how it works, with
validated data.
- Most of the papers that are presented are theories or principles that show some level of data augmentation. Although it's clear that, even with the core mathematical knolwedge, generating highly detailed faces, requires further
understanding.
- Further dive in the mathematics required, i.e the proof, as i got confused reading it, meaning that i need to brush up
on my calculus, and come back to this, next week.

# Self observation, and What Next

This week, was quite simple week, and it followed above the scope of the original project plan.
I noticed that i spent more time trying to understand the mathematical concepts behind the network more than anything,
and found that i had some gaps in my knowledge.

Hence there is no point trying to understand the maths if I do not know what its trying to represent; these gaps in my knowledge must be fixed in due time.

Following onwards from the original plan, I am not only going to look into more papers about how labeling works but
i want to look over my old A level notes and re evaluate my knolwedge, this is going to be nice, as not only will this
help my final year project, but im sure it will provide fruitful results towards my machine learning module as well.
